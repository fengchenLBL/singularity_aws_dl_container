{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using AWS Deep Learning Docker Containers on Singularity\n",
    "\n",
    "A modified version of this AWS SageMaker lab guide: https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Singularity container using available aws deep learning docker containers Images\n",
    "https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/\n",
    "\n",
    "The following shell code shows how to build the container image using `docker` and convert the container image to a `Singularity` image. \n",
    "\n",
    "### Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd container\n",
    "\n",
    "####################################################\n",
    "########## Download and unzip the dataset ##########\n",
    "####################################################\n",
    "cd ../data/\n",
    "wget https://danilop.s3-eu-west-1.amazonaws.com/reInvent-Workshop-Data-Backup.zip && unzip reInvent-Workshop-Data-Backup.zip\n",
    "mv reInvent-Workshop-Data-Backup/* ./\n",
    "rm -rf reInvent-Workshop-Data-Backup reInvent-Workshop-Data-Backup.zip\n",
    "cd ../container/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the SageMaker Container & Convert it to Singularity image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  456.3MB\n",
      "Step 1/9 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04\n",
      " ---> e6a210ff54e4\n",
      "Step 2/9 : RUN apt-get update &&     apt-get install -y nginx imagemagick graphviz\n",
      " ---> Using cache\n",
      " ---> 32ff2dce1af3\n",
      "Step 3/9 : RUN pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 4e1b65ea3a65\n",
      "Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm\n",
      " ---> Using cache\n",
      " ---> d97c22f6de86\n",
      "Step 5/9 : RUN python -m spacy download en_core_web_sm\n",
      " ---> Using cache\n",
      " ---> 14c8854a1901\n",
      "Step 6/9 : RUN python -m spacy download en\n",
      " ---> Using cache\n",
      " ---> 185661d9e15d\n",
      "Step 7/9 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> b5d5c6867074\n",
      "Step 8/9 : COPY sagemaker_keras_text_classification /opt/program\n",
      " ---> Using cache\n",
      " ---> ac73b50bd646\n",
      "Step 9/9 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> c5fe52a83024\n",
      "Successfully built c5fe52a83024\n",
      "Successfully tagged sagemaker-keras-text-classification:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "\u001b[34mINFO:   \u001b[0m Starting build...\n",
      "Getting image source signatures\n",
      "Copying blob sha256:f749b9b0fb213e9897417a985aaa9753d41bff474e1d0c0d1d266c4512eaf031\n",
      "Copying blob sha256:2558e637fbff95178cb4b43e0ca5f20a04ddeaf9673053bfa4dc10c72833d15a\n",
      "Copying blob sha256:aeda103e78c90b573700d64f6660efda378b59fe3e636ebfa28a0a105e2e2168\n",
      "Copying blob sha256:e79142719515e5304607fdd9adeb31db96b7acf00cabadac2678b056ed83bca6\n",
      "Copying blob sha256:9d2fda619715fb1f04019aab97191889de8648aaffde53347801b48bfbc8619e\n",
      "Copying blob sha256:7083756ef61fef3e835676de491bd26271ffd4812c2cc54f83336176e2c9745e\n",
      "Copying blob sha256:8722c9641a57d3bd9e09e3a3bd09d44354775543a363bc2b8d9c80ea23d583f5\n",
      "Copying blob sha256:d456742927ee9aab70e2c1ed4a27c56cc58dde0104de93a514e81dbfbd256908\n",
      "Copying blob sha256:0cf88c3675cd386365aea9dd2f9a70d7e481ff67534f3d510cc00d5e32bb615b\n",
      "Copying blob sha256:11fc4467b8a36b9752c60038f3dba5e25035c1642c7d63a1406fab6d8c860936\n",
      "Copying blob sha256:708ade65e14715074db61ed11a0c104151a8c2a919b79bdd34f19e67ea13c9ab\n",
      "Copying blob sha256:e722e212cbab10cdce87ac1ab6bba9ffb037d53b820b6f3a893269e5e44b6910\n",
      "Copying blob sha256:9ee6d909e5a76916033d9518a5b7629b70f8a24c4da2d5b8d87633f680e78886\n",
      "Copying blob sha256:b6e9883adafa4e135054c2fb120159e0f7c531ee902a1134dd691d68888ae094\n",
      "Copying blob sha256:b4064660a4cf9c71b108cdfff45efd4f1a58cf5af8c80ad5aae34401eaff1a58\n",
      "Copying blob sha256:cb460459ddc81fb4646e7e29d4f6b0afb3ec53a859d40111c79d5734ebea1608\n",
      "Copying blob sha256:3a97a8d562fbc124ae81d969f017a471de71b1926e52c6fe175b645b13652664\n",
      "Copying blob sha256:cc978a7bbd2a38f933e01857978ab1d7ae1102583742151e5a96580d79c9caa8\n",
      "Copying blob sha256:5bea870e21458c71b01e1b11de2389bee01838e1e8beca0a2c65dc9974402cf7\n",
      "Copying blob sha256:c745c040b5c56a4833b18201d8c89306a72aef1e43bcae357a93c806d484ffc4\n",
      "Copying blob sha256:06285212daf43857e18a5bda0d17d0b954c9976ec2e6f5b2d8297c883dd9d344\n",
      "Copying blob sha256:8cd641784060c4370f3c65ea703807cd20b51eab3d9a70434b144ae21afe63be\n",
      "Copying blob sha256:86b5ea61c34d1c1a8fc76d870b92b199ab54e7afd88073f2fed2b8b6f24fd5cf\n",
      "Copying blob sha256:87e513ddb4a6ce37dabf3de74b0284d49e08f4d7a3f0de393e6a533577e00f11\n",
      "Copying config sha256:77b2a54a3da8891391f609455182127c0944edb40397fbaf24f9ec80a9be5460\n",
      "Writing manifest to image destination\n",
      "Storing signatures\n",
      "2020/06/08 22:14:22  info unpack layer: sha256:bfa832b9344fc0c34a6491b769f9ef4ac95981dffc063b55b56a7976b44d9727\n",
      "2020/06/08 22:14:24  info unpack layer: sha256:849ddeb25733a3a84f8e34a2e00fde079a1fcf509e43a85aeffc8d377f00e77a\n",
      "2020/06/08 22:14:24  info unpack layer: sha256:5d4dd7f1d368b1459ee4ce6023264f7a61d4ca1278e1daf590ddf0ced1506fbb\n",
      "2020/06/08 22:14:24  info unpack layer: sha256:233fa1dcb029f17659d2f49813e89000d5ececaa72da46d2684b8d0b46a49872\n",
      "2020/06/08 22:14:24  info unpack layer: sha256:72ee6a2cf19f5701bdc02ec71c9e41564edfbb5547f78f3202fc1186137a3ddb\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:682ddba0812126442e7c4238c732347cd3c34caf379876b7df6c41d17dd60590\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:d1ea7e75c54a781f847c7651200b9ab7d6b10df7579c2b3e104e14ce5ae717e0\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:37445764563cd4e7996749b7e26a82d154fd14817bd06f71f8eb6c508f32745a\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:f0475fb4ced71280527710b7f701232e2991d1c25e63377c6de78397a7512edb\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:4ee3c68d995fbc5d079b44891f1beff47fd9ea8318431bda16353affd656d338\n",
      "2020/06/08 22:14:29  info unpack layer: sha256:431b329ccde61e0bd299d8536171589e70b34465c18f80fbe96a80826ea3cb3c\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:6969e5183761812046d944a000d88ae794604c22dd7fd58679f5a9463bb2b016\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:23f94511f4647c10f03287e3fbe9c2eb5266d29deeea8f4ebafc1103c782e13d\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:17a195689366e680cc60838296db56ac5e54e35b4c373aae418f5e4c5c654b57\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:9d42b79972b1d20d9d96686270a190b476d7cac3c38cf97ca6783573d20b8460\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:7df3073fc076d71acb24f7e509aad6744ff88f1771e766aa25daf5da4104fbf5\n",
      "2020/06/08 22:14:33  info unpack layer: sha256:eee47f98847d7cc2d1317512855cc5c141a4ca10ccca1402b20fd477e7d37970\n",
      "2020/06/08 22:14:45  info unpack layer: sha256:1d59c56290ed49ca4166f283e50f9e9fdde598b8347aaa43132615f0a659c24d\n",
      "2020/06/08 22:14:45  info unpack layer: sha256:382a9e3fc8c959211778d5ea4bd6e5144ff7913a0f37d9ec30d7c1ff31b5d524\n",
      "2020/06/08 22:14:47  info unpack layer: sha256:39a37be9c7ae6436ee8d987d245f11074e403f4d4403504f06c17dc513657fed\n",
      "2020/06/08 22:14:47  info unpack layer: sha256:647dce8a9de5ada5719e82c2ff5408867fcaa83145665bea4103d3705c2326b1\n",
      "2020/06/08 22:14:49  info unpack layer: sha256:1df727cf7f1435f496890edded1650193af403065eff27929a8b374d5b36d743\n",
      "2020/06/08 22:14:49  info unpack layer: sha256:df2ccfca12a78a5c880fd30514c57c84f250a81c223915e124cff93833f6b5d2\n",
      "2020/06/08 22:14:49  info unpack layer: sha256:88f2c64e66817e60a415e82323d1a2d3f19ca75eb4ea9ae7692a2fccc09c2de5\n",
      "\u001b[34mINFO:   \u001b[0m Creating SIF file...\n",
      "\u001b[34mINFO:   \u001b[0m Build complete: local_sagemaker-keras-text-classification.sif\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd container\n",
    "\n",
    "###################################################################################\n",
    "######### Build the SageMaker Container & Convert it to Singularity image #########\n",
    "###################################################################################\n",
    "algorithm_name=sagemaker-keras-text-classification\n",
    "\n",
    "chmod +x sagemaker_keras_text_classification/train\n",
    "chmod +x sagemaker_keras_text_classification/serve\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "fullname=\"local_${algorithm_name}:latest\"\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884)\n",
    "\n",
    "# Build the docker image locally with the image name\n",
    "# In the \"Dockerfile\", modify the source image to select one of the available deep learning docker containers images:\n",
    "# https://aws.amazon.com/releasenotes/available-deep-learning-containers-images\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "# Build Singularity image from local docker image\n",
    "sifname=\"local_sagemaker-keras-text-classification.sif\"\n",
    "sudo singularity build ${sifname} docker-daemon:${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training.\n",
      "                                               TITLE  ...      TIMESTAMP\n",
      "1  Fed official says weak data caused by weather,...  ...  1394470370698\n",
      "2  Fed's Charles Plosser sees high bar for change...  ...  1394470371207\n",
      "3  US open: Stocks fall after Fed official hints ...  ...  1394470371550\n",
      "4  Fed risks falling 'behind the curve', Charles ...  ...  1394470371793\n",
      "5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...  ...  1394470372027\n",
      "\n",
      "[5 rows x 7 columns]\n",
      "Found 65990 unique tokens.\n",
      "Shape of data tensor: (422417, 100)\n",
      "Shape of label tensor: (422417, 4)\n",
      "x_train shape:  (337933, 100)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 20002     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 12        \n",
      "=================================================================\n",
      "Total params: 1,020,014\n",
      "Trainable params: 1,020,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 337933 samples, validate on 84484 samples\n",
      "Epoch 1/6\n",
      "337933/337933 - 25s - loss: 0.6788 - acc: 0.7409 - val_loss: 0.6146 - val_acc: 0.7757\n",
      "Epoch 2/6\n",
      "337933/337933 - 25s - loss: 0.5958 - acc: 0.7824 - val_loss: 0.5889 - val_acc: 0.7840\n",
      "Epoch 3/6\n",
      "337933/337933 - 25s - loss: 0.5778 - acc: 0.7882 - val_loss: 0.5755 - val_acc: 0.7893\n",
      "Epoch 4/6\n",
      "337933/337933 - 25s - loss: 0.5707 - acc: 0.7904 - val_loss: 0.5697 - val_acc: 0.7918\n",
      "Epoch 5/6\n",
      "337933/337933 - 25s - loss: 0.5673 - acc: 0.7918 - val_loss: 0.5684 - val_acc: 0.7915\n",
      "Epoch 6/6\n",
      "337933/337933 - 25s - loss: 0.5648 - acc: 0.7920 - val_loss: 0.5657 - val_acc: 0.7923\n",
      "Training complete. Now saving model to:  /opt/ml/model\n",
      "Test headline:  What Improved Tech Means for Electric, Self-Driving and Flying Cars\n",
      "Predicted category:  t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'test_dir/output/*': No such file or directory\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2020-06-08 22:24:19.850096: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-08 22:24:19.875293: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300060000 Hz\n",
      "2020-06-08 22:24:19.875595: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x372a5f0 executing computations on platform Host. Devices:\n",
      "2020-06-08 22:24:19.875642: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-06-08 22:24:19.887967: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd container\n",
    "\n",
    "################################\n",
    "########## Local Test ########## \n",
    "################################\n",
    "cd ../data\n",
    "cp -a . ../container/local_test/test_dir/input/data/training/\n",
    "cd ../container\n",
    "cd local_test\n",
    "\n",
    "### Train\n",
    "sifname=\"local_sagemaker-keras-text-classification.sif\"\n",
    "./train_local.sh ../${sifname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
